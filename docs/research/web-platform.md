---
title: Платформа HPCCloud
---

<style>
img[src$="centerme"] {
  display:block;
  margin: 0 auto;
}
</style>

# Платформа HPCCloud

Проект HPCCloud стартовал в 2014 году, разработка которой происходит постепенно. В настоящий момент проект не имеет какой-то конечной реализации, некоторые функции разработаны полноценно, а другие нет.

Так или иначе, платформа позволяет пользователю создавать расчетные задачи для моделирования, начиная с введения параметров и заканчивая последующей постобработкой.

Для запуска задач создаются проекты, в которых запускаемые расчеты можно группировать по выбранному пакету (напр., OpenFoam) или по начальным данным: по коэффициентам, геометрии сетки и т.п.

HPCCloud поставляется с несколькими встроенными наборами инструкций, которые позволяют запускать задачи в пакетах PyFR [20] или OpenFoam [21]. Визуализация результирующих данных происходит с помощью ParaViewWeb [27].

## Проекты и моделирование

HPCCloud использует концепцию проектов для запуска моделирования. Проект также определяет конкретный пакет, который будет использоваться для моделирования.

Задачи моделирования создаются внутри проекта, так как они являются экземплярами определенного типа пакета моделирования. Таким образом, добавление нового рабочего процесса моделирования в HPCCloud включает реализацию нового типа проекта и связанного с ним моделирования.

## Доступ к вычислительным ресурсам

Сам процессор моделирования должен выполняться на вычислительном ресурсе. HPCCloud поддерживает два типа ресурсов: «Традиционные» кластеры — это то, что большинство людей считают вычислительными ресурсами. Это специализированные статически подготовленные машины. HPCCloud также предоставляет возможность создавать так же динамические кластеры в Amazon Elastic Compute Cloud (EC2). Процессы моделирования HPCCloud переносимы между ресурсами HPC. Таким образом, один и тот же рабочий процесс может выполняться как в традиционном кластере, так и в облачном кластере. Инфраструктура HPCCloud защищает разработчика рабочих процессов от многих различий между этими двумя типами ресурсов.

«Традиционные» вычислительные кластеры. HPCCloud получает доступ к традиционным кластерам, используя SSH. Для запуска рабочих процессов моделирования в кластере необходимо настроить пару ключей, чтобы обеспечить безопасный доступ HPCCloud к кластеру (см. рис. ). Стоит обратить внимание что кластер должен поддерживать SSH-доступ на основе ключей.

![Alt](/hpccloud-kemsu/research__images/7j6bbt.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Пользовательский интерфейс для настройки "традиционного" кластера</i></center>

Amazon Elastic Compute Cloud (EC2) кластеры. Для создания экземпляров EC2 требуются учетные данные для доступа к соответствующей учетной записи. HPCCloud использует профиль в Amazon Web Services (AWS) для управления этими учетными данными (см. рис. ). Профиль содержит ключ доступа AWS и информацию о регионе, необходимую для подключения к AWS. Этот ключ доступа будет использоваться HPCCloud для аутентификации, чтобы взаимодействовать с API сервисов AWS.

![Alt](/hpccloud-kemsu/research__images/VKbMR7.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Пользовательский интерфейс для настройки кластера в AWS</i></center>

## Запуск процесса моделирования

В официальной документации HPCCloud можно встретить термин «Workflow», означающий поток работ или процесс моделирования. Например, процессы моделирования для пакетов OpenFoam и PyFR будут разными, поэтому команды в инструкциях прописываются иначе. Директории с результатами могут располагаться уникальным образом.

Для каждого пакета численного моделирования создавать процессы моделирования (workflow), состоящий из трех основных этапов:

1. Генерирование начальных параметров - Компонент Simput [26] сгенерирует панель ввода для моделирования.
2. Процессор моделирования – файлы для запуска расчетной задачи будут размещены на кластере и после производиться мониторинг.
3. Визуализация - постобработка полученных результатов, используя ParaViewWeb [27] (важно, чтобы пакет поддерживал ParaView форматы).

Для моделирования в HPCCLoud необходимо выполнить следующие действия:

1. Создать проект и выбрать тип «pyfr». Назвать созданный проект, а затем загрузить входной файл сетки (щелкнуть правой кнопкой мыши и «Сохранить ссылку как…», чтобы загрузить).

2. Создать моделирование и написать ее наименование.

3. Перейдите к созданному моделированию и нажать на шаг «Input» на левой боковой панели.

4. Следуйте инструкциям Simput для PyFr, которые изложены в документах HPCCloud.

5. Открыть раздел «Simulation» в левой боковой панели, затем выбрать кластер.

6. Нажмите «Run simulation», чтобы перейти к подэтапу «Представление симуляции». В этом разделе просматривается мониторинг выполнения задач. Когда все они достигают статуса «Завершено», то результаты моделирования закончены. Появится кнопка «Visualization». Нажав на нее, будет переход к следующему разделу.

![Alt](/hpccloud-kemsu/research__images/lxr5Ub.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Мониторинг задачи в HPCCloud</i></center>

7.  На шаге «Visualization» необходимо выбрать сервер и нажать на «Run visualization». Он перейдет на страницу мониторинга, аналогичную странице «Simulation», описанного в шаге 6. 8) После запуска визуализации ParaViewWeb [27], появится кнопка с надписью «Visualization». Нажав на эту кнопку, будет переход к инструменту ParaViewWeb в браузере, где интерактивно можно анализировать результаты моделирования.

![Alt](/hpccloud-kemsu/research__images/VFxfme.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Визуализация результирующих данных в HPCCloud</i></center>

## Архитектура HPCCloud

Архитектуру HPCCloud можно разделить на разные части (см. рис. ). Основной частью является серверное приложение на Girder, которое отвечает за хранение данных и мониторинг выполнения расчётных задач.

Клиентское приложение реализует пользовательский интерфейс платформы и взаимодействие с сервером по протоколам HTTP и WebSocket [6]. Брокер сообщений RabbitMQ и воркер задач Celery выполняют асинхронные и трудоемкие запросы на вычислительный кластер.

![Alt](/hpccloud-kemsu/research__images/nTP7xR.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Архитектура HPCCloud</i></center>

Рассмотрев кратко архитектуру, поговорим про каждую часть архитектуры в следующих параграфах данной главы.

## Платформа Girder, как фреймворк для серверной приложения

Girder (см. рис. ) — это свободно-распространяемая веб-платформа управления данными с открытым исходным кодом для обработки данных и аналитики [15]. Это отдельная платформа или фреймворк для создания новых веб-сервисов.

![Alt](/hpccloud-kemsu/research__images/z6uBlO.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Пользовательский интерфейс Girder</i></center>

Он предназначен для быстрого и простого создания веб-приложений, которые имеют некоторые или все из следующих требований:

1. Организация данных. Многие веб-приложения должны управлять данными, которые динамически предоставляются пользователями системы или предоставляются через внешние службы данных. Girder упрощает построение и организацию динамических иерархий данных. Одним из наиболее мощных аспектов Girder является то, что он может прозрачно хранить, обслуживать и передавать данные из гетерогенных механизмов внутреннего хранилища через единый RESTful API, включая локальные файловые системы, базы данных MongoDB, хранилища значений ключей, совместимых с Amazon S3. Распределенные файловые системы (HDFS).

2. Управление пользователями и аутентификация. Girder также включает в себя все необходимое для управления подключаемыми пользователями и аутентификации из коробки и придерживается лучших практик в области веб-безопасности. Система может быть сконфигурирована для безопасного хранения учетных данных сама или для передачи сторонним службам аутентификации, таким как OAuth или LDAP.

3. Управление Доступом. Girder поддерживает простую схему управления правами, которая позволяет управлять доступом как на основе пользователя, так и на основе ролей к ресурсам, управляемым в системе. Проект прошел строгий аудит безопасности и имеет обширное автоматизированное тестирование для проверки правильности поведения и обеспечения правильности.

Серверная архитектура Girder (см. рис. ) ориентирована на создание RESTful API, чтобы обеспечить минимальную связь между серверными сервисами и внешними клиентами. Такое разделение позволяет нескольким клиентам использовать один и тот же интерфейс на стороне сервера.

Хотя Girder содержит свое собственное одностраничное веб-приложение на javascript, система может использоваться любым клиентом с поддержкой HTTP как внутри, так и вне среды веб-браузера. Girder может даже работать без внешнего приложения, только обслуживая маршруты API.

![Alt](/hpccloud-kemsu/research__images/id1MK0.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Архитектура Girder</i></center>

API в основном используется для взаимодействия с ресурсами, которые представлены моделями в системе. Модели внутренне взаимодействуют с базой данных MongoDB для хранения и извлечения постоянных записей. Модели содержат методы для создания, изменения, извлечения и удаления этих записей.

Основной метод настройки и расширения Girder заключается в разработке плагинов (расширения). Плагины, например, могут добавлять новые маршруты для REST API, изменять или удалять существующие, обслуживать другое веб-приложение из корня сервера, подключаться к событиям жизненного цикла модели или определенным вызовам API переопределять поведение аутентификации для поддержки новых сервисов или протоколов аутентификации, добавлять новый внутренний механизм хранения для файлов или даже взаимодействие с совершенно другой СУБД для сохранения системных записей.

Плагины содержатся в своем собственной директории, независимо от исходной директории Girder. Поэтому они могут находиться в своем собственном отдельном репозитории исходных кодов и устанавливаются путем простого копирования дерева исходных кодов плагинов в каталог плагинов существующей установки Girder. Репозиторий Girder содержит несколько обычно полезных плагинов в исходной поставке.

## Организация данных и доступ к ним

В течение жизненного цикла рабочего процесса HPC необходимо сохранить широкий спектр ресурсов. Они варьируются от входящей конфигурацией кластера до выходных наборов данных и связанных c ними метаданными. Эти данные должны быть не только сохранены, но и должны применять соответствующие средства контроля доступа.

Girder может прозрачно хранить, обслуживать и передавать данные, хранящиеся в нескольких различных хранилищах, включая локальные файловые системы, базы данных MongoDB, хранилища значений ключей, совместимые с Amazon S3.

Абстракция хранилища активов скрывает детали базовой системы хранения данных, обеспечивая единый API для многих разнородных технологий. cumulus использует инфраструктуру плагинов Girder для дополнения существующей модели данных моделями, специфичными для рабочих процессов HPC, такими как кластер и модель задания:

- Кластер – это модель, управляемая системой. Содержит данные конфигурации, а также информацию времени выполнения, такую как текущее состояние кластера и журнал событий, связанных с операциями кластера.
- Задание – это модель тяжелого задания, работающее или запускаемое в кластере. Записывает сведения о задании, такие как сценарий отправки, а также состояние выполнения.

Также в Girder имеется специализированное хранилище ресурсов плагинов, которое предоставляет возможность интеллектуального управления файлами, которые хранятся в файловой системе кластера. Вместо наивного подхода загрузки всех выходных данных, связанных с выполнением задания, Cumulus загружает в хранилище ресурсов только метаданные, такие как имя файла, размер и путь в файловой системе кластера. Для клиентского приложения эти файлы отображаются так же, как и любой другой файл, хранящийся в Girder.

До тех пор, пока клиент не попытается получить доступ к контенту, передача данных будет фактически выполнена. Хранилище активов содержит ссылку на учетные данные, необходимые для доступа к кластеру. Они используются для установления SFTP-соединения [23] с кластером.

Затем содержимое файла может быть передано из кластера через это соединение и обратно к клиенту через соединение HTTP. По сути, SFTP-соединение связано с HTTP-соединением клиента. Хотя этот подход не решает проблему передачи больших объемов данных по HTTP, он пытается минимизировать объем данных, передаваемых только тем данным, которые интересуют клиента. Оставление данных в кластере также означает, что последующие шаги в Рабочий процесс HPC также может работать на нем. Типичным примером этого является этап визуализации постобработки, который использует ParaView [27], который может выполняться на том же ресурсе HPC.

## Расширение Cumulus

Для среды моделирования важно управлять и производить мониторинг над кластерами. Такой функциональностью обладает плагин Cumulus.

![Alt](/hpccloud-kemsu/research__images/cnYp0m.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Клиент-серверное взаимодействие в Cumulus</i></center>

Cumulus расширяет базу бизнес-логики Girder следующими объектами: конфигурации кластеров, кластеры, сценарии, задания и задачи. Все конечные точки REST API являются асинхронными, поэтому любые долго ожидающие задачи делегируются в Celery, который отвечает за распределенную очередь заданий (distributed task queue) [22]. Распределенная очередь означает следующее:

- Выполнять задания асинхронно или синхронно.
- Выполнять периодические задания (аналог crond).
- Выполнять отложенные задания.
- Распределенное выполнение (может быть запущен на N серверах).
- В пределах одного worker'а возможно конкурентное выполнение нескольких задач(одновременно).
- Выполнять задание повторно, если сработал exception.
- Ограничивать количество заданий в единицу времени (rate limit, для задания или глобально).
- Мониторить выполнение заданий
- Выполнять подзадания
- Присылать отчеты об выполнении exception на email
- Проверять выполнилось ли задание

Для Celery настроен брокер сообщений (диспетчер очереди) RabbitMQ, чтобы производить клиент-серверную коммуникацию между вычислительными ресурсами. В RabbitMQ используется расширенный протокол очереди сообщений (AMQP). Брокер RabbitMQ используется для хранения очереди сообщений (задач) [25].

Cumulus обеспечивает стабильную работу при длительных асинхронных запросов и предотвращает платформу от невосприимчивости.

## Планирование заданий в системы управления

Cumulus отправляет задания в ресурсы HPC с использованием планировщиков заданий, как для традиционных кластеров, так и для кластеров, созданных в облаке. Облачный кластер часто предназначен для выполнения одного вычислительного задания, поэтому на первый взгляд может быть неясно, какие преимущества дает планировщик заданий в такой среде. Тем не менее, планировщик заданий позволяет унифицировать отправку заданий и отслеживать код во всех кластерах, которыми управляется через единый интерфейс. Это также позволяет отправлять несколько заданий в один и тот же облачный кластер.

Cumulus предоставляет интерфейс адаптера, который определяет основные операции, которые обеспечивает планировщик, такие как отправка, завершение и мониторинг. Эта абстракция позволяет платформе поддерживать несколько планировщиков, включая SGE, PBS и Slurm. Поддержка новых планировщиков может быть обеспечена путем добавления дополнительных реализаций интерфейса адаптера.

Cumulus полностью обращается к ресурсам HPC через SSH с использованием аутентификации на основе ключей. Ресурсы HPC, созданные с использованием среды Amazon EC2, используют пары ключей, сгенерированные с использованием API EC2 [41]. Закрытый ключ загружается и надежно хранится в файловой системе с помощью Cumulus, а EC2 заботится о том, чтобы открытый ключ добавлялся в случаи запуска кластера. Это гарантирует, что Cumulus имеет безопасный доступ к кластеру. В случае традиционных кластеров Cumulus отвечает за создание пары ключей. Открытый ключ предоставляется, как часть конфигурации кластера. Однако пользователь сам должен добавить этот ключ в список авторизованных ключей учетной записи кластера, к которым он хочет получить доступ к куче.

Закрытый ключ шифруется и записывается в файловую систему. Ключевая фраза хранится в Girder, но она никогда не раскрывается конечной точкой RESTful. SSH предоставляет Cumulus стандартный, безопасный интерфейс для широкого спектра общедоступных, частных традиционных и облачных ресурсов HPC.

## Принцип процессов моделирования

Рабочие процессы HPC могут варьироваться от очень простых линейных потоков, которые содержат несколько шагов, до более сложных потоков, которые содержат ветви и даже циклы. Для поддержки этих рабочих процессов необходим эффективный механизм для выполнения требуемой работы и отслеживания результирующего состояния. Важной особенностью этих рабочих процессов является то, что они потенциально могут быть очень долгим.

Расчетные коды могут занять несколько часов или дней. Важно, чтобы для выполнения рабочего процесса использовались минимальные ресурсы в ожидании завершения этих длительных заданий. Чтобы удовлетворить эти конкретные требования, в Cumulus был разработан механизм рабочего процесса, построенный поверх Celery.

Celery - это асинхронная очередь задач / заданий с открытым исходным кодом, основанная на распределенной передаче сообщений. Это реализовано в Python. Единицей выполнения называют задачей, которая отображается на функцию Python. Задача запланирована для выполнения путем помещения сообщения в очередь, и рабочие процессы конкурируют за выборку сообщений из очереди и последующим выполнением задачи.

Этот простой шаблон позволяет эффективно линейно масштабировать, добавляя новые рабочие процессы. Используя Celery, создали простой, но мощный механизм рабочих процессов под названием TaskFlow, чтобы обеспечить выполнение рабочих процессов HPC. Используются существующие конструкции рабочего процесса Celery, чтобы позволить группировать задачи Celery в поток, состояние которого поддерживается и контролируются конечными точками RESTful Girder.

Как и в ванильных приложениях Celery, задачи отображаются на функции Python. Однако в задачах TaskFlow применяется Python-декоратор для учета, необходимого для связи задачи Celery с конкретным TaskFlow, а также для синхронизации состояния в рамках модели Girder.

![Alt](/hpccloud-kemsu/research__images/z9iHQZ.png?style=centerme)

<center style="margin: -25px 0px 25px;"><i>По мере выполнения задач Celery соответствующие объекты создаются в модели Girder, поэтому состояние и ход TaskFlow сохраняются. Затем информация может быть передана клиентам через RESTful API.</i></center>

На рисунке показано, как состояние задач TaskFlow отображается в Girder. Поскольку каждая задача запускается в Celery, декоратор обращается к Girder с просьбой создать соответствующую задачу в своей модели данных и связать ее с текущим TaskFlow. Этот объект задачи представляет это состояние работающей задачи Celery. По завершении задачи Celery, в Girder производится запрос на обновление состояния соответствующего объекта задачи. Таким образом, состояние набора задач Celery, связанных с TaskFlow, отражается в модели данных Girder. На рисунке показана схема кода TaskFlow.

![Alt](/hpccloud-kemsu/research__images/z66Og7.png?style=centerme)

<center style="margin: -5px 0px 25px;"><i>Схема кода простого TaskFlow показывает, как задачи объединяются в цепочки для формирования рабочего процесса.</i></center>

Статус TaskFlow определяется состоянием отдельных задач, которыми он в себя включает. Набор маршрутов REST предоставляется для обновления состояния. Например, пользователь может запросить, какие задачи в настоящее время являются частью TaskFlow, или состояние отдельных задач. TaskFlow и связанные с ними задачи могут также содержать метаданные, такие как информация журналирования, записанная при выполнении кода Python, реализующего задачу.

Так же стоит упомянуть Simput, это инструмент для упрощения процесса написания и редактирования входных файлов моделирования. Это может быть автономный инструмент, но для HPCCloud он интегрирован для поддержки генерации входных данных для моделирования, такого как PyFR или OpenFoam.
Simput отвечает за препроцессинг моделирования, Cumulus и Celery отвечают за запуск и мониторинг на вычислительном кластере (см. рис. ).

![Alt](/hpccloud-kemsu/research__images/dwxWKh.png?style=centerme)

<center style="margin: -5px 0px 25px;"><i>Процессы моделирования в HPCCloud</i></center>

## Клиентская часть

Клиентская часть (front-end) платформы является довольно-таки важной частью в управлении всех многих процессов. Имеется в виду то, что серверная часть хоть ответственная за большинство операций: хранение данных, делегирование разных асинхронных задач, работа с кластером и т.п.; Предоставляет внешний API для клиентской части, в которой реализованы принципы работы с этим серверным приложением. То есть около 40-60% кодовой базы платформы уделяется клиентской части. На рисунке показана архитектура клиентского приложения.

Клиентское приложение разработано на React.js, который является фреймворком для создания пользовательских интерфейсов. В целом включает в себя следующий библиотеки:

- React – Фреймворк приложения
- Redux – реализует глобальное состояния
- React-Router – маршрутизация приложений
- PostCSS – инструмент, которй автоматизирует рутинные операции с CSS с помощью расширений
- Axios – Библиотека для работы с серверными запросами
- FontAwesome – библиотека иконок

![Alt](/hpccloud-kemsu/research__images/Mtqjcb.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Архитектура клиентского приложения</i></center>

## Удаленная визуализация

ParaViewWeb — это фреймворк на языке JavaScript, в котором есть модули и компоненты для создания веб-приложений с интерактивной научной визуализацией в веб-браузере.

![Alt](/hpccloud-kemsu/research__images/68xDHE.jpg?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Архитектура ParaViewWeb</i></center>

ParaViewWeb представляет собой набор повторно используемых компонентов. Эти компоненты варьируются от стороны сервера до стороны клиента. На стороне сервера рендеринга (PVServer) представляется механизм взаимодействия с пакетом ParaView, который выполняет фактическую визуализацию. Он может подключиться к удаленному серверу обработки ParaView Server, который может производить параллельные вычисления на стороне кластера c помощью MPI. Затем компонент веб-службы с именем PWService управляет связью между удаленными серверами визуализации (PWServer) и клиентами.

На стороне клиента предоставляется библиотека JavaScript для создания удаленных визуализаций и управления ими, а также несколько компонентов визуализации, позволяющих пользователям интерактивно просматривать трехмерный контент в браузере. Используя эти компоненты, разработчики могут создавать веб-сайты или веб-порталы с возможностями визуализации и обработки данных. Эти компоненты могут быть легко интегрированы в многофункциональные интернет-приложения, разработанные с использованием популярных инфраструктур веб-проектирования.

В контексте платформы HPCCloud визуализация производится с помощью инструмента ParaView Visualizer, разработанного на основе фреймворка ParaViewWeb.

## Режимы подключения к удаленной визуализации

Веб-приложения на ParaViewWeb могут так же взаимодействовать с локально установленным ParaView. На рисунке ниже показано, как отдельный пользователь может начать взаимодействие с локальным экземпляром ParaViewWeb.

![Alt](/hpccloud-kemsu/research__images/12hGWU.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Однопользовательский режим</i></center>

В этом случае ParaViewWeb — это отдельный сценарий, который может быть выполнен предоставленным интерпретатором Python. Сценарий будет отвечать за запуск веб-сервера и прослушивание данного порта.
Эта настройка не позволяет нескольким пользователям подключаться к удаленному серверу и создавать свои собственные визуализации независимо друг от друга. Для такой поддержки необходима настройка многопользовательского режима.
Чтобы поддерживать соединение нескольких пользователей в разных сеансах визуализации в ParaView, сервер должен предоставить единую точку входа для установления соединения, а также механизм для запуска нового сеанса визуализации.
На приведенном ниже рисунке показана такая настройка, в которой Apache используется в качестве веб-сервера для клиентской части, а также для установления WebSocket-соединения в соответствующий сеанс визуализации. Кроме того, процесс запуска используется для динамического запуска процесса pvpython для сеанса визуализации.

![Alt](/hpccloud-kemsu/research__images/72oQND.png?style=centerme)

<center style="margin: -15px 0px 25px;"><i>Многопользовательский режим</i></center>

Конфигурация многопользовательского режима на порядок сложнее, чем однопользовательский, но является практичной для полноценной работы приложения, чтобы поддерживать более одного пользователя.
На рисунке Рис. 3 описаны три компонента, необходимые для развертывания ParaViewWeb в многопользовательской среде. Эти три компонента изображены на следующем рисунке и включают в себя: 1. Клиентская часть (Frontend), который является единой точкой входа для всех клиентов; 2. Модуль запуска (Launcher), который может запустить новый процесс визуализации для каждого клиента; 3. Веб-сервер ParaViewWeb, который взаимодействует с пакетом ParaView.

![Alt](/hpccloud-kemsu/research__images/pvw-3comp-resize.png?style=centerme)

<center style="margin: -15px 0px 25px;"><i>Компоненты в многопользовательском режиме</i></center>

Задача клиентской части - обслуживать статический контент, а также выполнять переадресацию по сети, чтобы была единая точка входа, через которую общаются все клиенты. Клиентская часть должна иметь возможность пересылать определенные запросы в модуль запуска, когда новый клиент начинает новый сеанс визуализации, а затем в клиентскую часть возвращать номер сессии через модуль запуска, чтобы отображать последующие запросы сеанса от этого клиента на порт, где сеанс визуализации клиентов прослушивается.

Модуль запуска (Launcher) отвечает за создание процесса визуализации ParaView для каждого пользователя, который запрашивает его, а также за передачу идентификатора сеанса и связанного номера порта внешнему компоненту. Это позволяет компоненту переднего плана знать, как перенаправлять будущие запросы от каждого клиента на правильный порт, где прослушивается сеанс визуализации этого клиента.

## ParaView Visualizer

Существуют разные официальные открыто разрабатываемые веб-инструменты на фреймворке ParaViewWeb. Для задач гидродинамики был разработан и интегрирован в веб-платформу инструмент ParaView Visualizer.

Веб-приложение Visualizer (см. рис. ) обеспечивает ParaView-подобный интерфейс в веб-браузере. Библиотека ParaViewWeb содержит все компоненты, необходимые для создания пользовательского интерфейса и подпрограмм доступа к данным для связи с сервером ParaView с использованием подключения WebSocket.

![Alt](/hpccloud-kemsu/research__images/W2CQDi.png?style=centerme)

<center style="margin: -15px 0px 25px;"><i>Пользовательский интерфейс Paraview Visualizer (независимо от веб-платформы)</i></center>

Пользовательский интерфейс реализован на фреймворке ReactJS. Библиотека ParaViewWeb содержит все компоненты, необходимые для создания пользовательского интерфейса и подпрограмм доступа к данным для связи с сервером ParaView с использованием подключения WebSocket.

## Развертывание с помощью Docker

Платформа включает в себя множество различных компонентов. Качественный запуск всей платформы «вручную» - задача нетривиальная, так как каждый компонент зависит один от другого, поэтому развёртывание осуществляется через программное обеспечение Docker [31], которое синхронно запускает по конфигурации каждый компонент в отдельно взятых виртуальных контейнерах (см. рис. ).

![Alt](/hpccloud-kemsu/research__images/x9CDtu.png?style=centerme)

<center style="margin: 0px 0px 25px;"><i>Клиент-серверное взаимодействие между виртуальными контейнерами</i></center>

Таким образом, разработчиками была подготовлена Docker-конфигурация, запускающая исходной код платформы HPCCloud. Главным недостатком данной конфигурации являлось то, что предназначена только для запуска так называемой production-версии проекта, поэтому необходимо было изучить все важные инструкции развёртывания и создать новую конфигурацию для разработки, исходной код которой выложен на Github [33].
